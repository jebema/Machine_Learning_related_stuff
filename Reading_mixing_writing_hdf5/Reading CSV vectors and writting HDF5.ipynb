{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Reading CSV files\n",
    "\n",
    "In my case, I have a folder containing vectors comming from the data. <br>\n",
    "Each vector is an econding or a feature obtained through pre-processing. The pre-processing could be done over images, videos, audio, etc. The pre-processing is out of the scope of this document. Indeed, sometimes is required to have the raw information. The raw data can also be reshaped to be a vector. \n",
    "\n",
    "The folder contains one Comma-Separated-Value (CSV) file for each class. Each CSV file contains rows of vectors. Each row could represent an image, a video segment, or something else. The next Figure depicts this scenario.\n",
    "\n",
    "Note: In my case I am actually not separating with comma but with the delimiter tab. <br>\n",
    "Note2: Recall that each files contains all the data from one class. Prior processing is required to put together all data from each class in one file as the ones used here. A similar procedure than in section 3 could be used to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Pre-processing_CSVFiles.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the libraries that we will be using: <br>\n",
    "csv - to use the writer <br>\n",
    "numpy - to convert to numpy array, this is not required but if some processing is needed could be helpful, such as framing <br>\n",
    "os - to read directories <br>\n",
    "math - to calculate values <br>\n",
    "random - to do shuffling <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries \n",
    "import csv            \n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, some functions that will be used. <br>\n",
    "get_number_of_lines_in_file - To know the number of rows in a text file, it is required to read all the file. <br>\n",
    "load_csv - Load all the file and save it in a List. <br>\n",
    "str_dataset_2_numpy_dataset - To convert the List into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function returns the number of rows in a CVS file\n",
    "def get_number_of_lines_in_file(fileName):\n",
    "    file_object = open(fileName)\n",
    "    row_count = sum(1 for row in file_object)\n",
    "    file_object.close()\n",
    "    return row_count\n",
    "\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'rt') as csvfile:\n",
    "        readData = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in readData:\n",
    "            dataset.append(row)    \n",
    "    return dataset\n",
    "\n",
    "# Convert string data to numpy matrix\n",
    "# This is not appropiate for a huge amount of data because it requires to load it all in an array\n",
    "def str_dataset_2_numpy_dataset(dataset):\n",
    "    Mat = np.zeros((int(len(dataset)),len(dataset[0])))\n",
    "    for i in range(len(dataset)):   \n",
    "        Mat[i] = np.array(dataset[i][:], dtype=float)\n",
    "    return Mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#Example of getting the number of rows in a file\n",
    "fileName = \"prueba/class_0.csv\"\n",
    "print(get_number_of_lines_in_file(fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of reading a file and converting to a numpy matrix\n",
    "dataset = load_csv(fileName)\n",
    "Mat = str_dataset_2_numpy_dataset(dataset)\n",
    "Mat.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Separating train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use a rutine to separate the vectors from each class to obtain train and test files. In this example I separate the data into 70% for train and 30% for test. The next Figure shows this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/separate_train_test.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code are instructions to separate each file into two files, the train and test files. <br>\n",
    "It first load the dataset, then it converts it to a numpy array. We get the number of elements and create a list with the number of elements that then we shuffle. We save the new files by traversing through the shuffled index and put 70% in the train file and 30% in the test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Train----------\n",
      "14\n",
      "------ Test-------\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#In this code, I have to put the file name. In this example I have 3 files and I would need to change the class name {1,2,3} \n",
    "#to perform the separation over all files. However, this can be automated using os.walk as shown ahead\n",
    "className = \"2\"  #{0,1 or 2 in this example}\n",
    "fileName = \"prueba/class_\" + className + \".csv\"\n",
    "fileNameTrain = \"prueba/train/class_\" + className + \".csv\"\n",
    "fileNameTest  =\"prueba/test/class_\" + className + \".csv\"\n",
    "\n",
    "#Load the data\n",
    "dataset = load_csv(fileName)\n",
    "\n",
    "#Convert the data to a numpy array (These functions could be merged and it would be possible to create the array since reading it)\n",
    "#without requiering to make first a list\n",
    "dataset_array = str_dataset_2_numpy_dataset(dataset)\n",
    "\n",
    "#Get the number of rows and columns\n",
    "len_file_csv,nCols = dataset_array.shape\n",
    "\n",
    "#Create an index starting from 0 to the number of rows\n",
    "index_shuffle = [i for i in range(len_file_csv)]\n",
    "\n",
    "#Shuffle the rows\n",
    "np.random.shuffle(index_shuffle)\n",
    "\n",
    "#Getting how many lines are going to be in the train file\n",
    "n_train =  math.floor(len_file_csv*.7)\n",
    "\n",
    "count = 0\n",
    "\n",
    "#Saves 70% of the shuffled rows into a file\n",
    "csvfile = open(fileNameTrain, 'wt', newline='')\n",
    "while (count < n_train):\n",
    "    MV_writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    MV_writer.writerow(dataset_array[index_shuffle[count]])\n",
    "    count += 1            \n",
    "csvfile.close()\n",
    "\n",
    "#Saves the other 30% of the shuffled rows into a file\n",
    "csvfile = open(fileNameTest, 'wt',newline='')\n",
    "while (count < len_file_csv):\n",
    "    MV_writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    MV_writer.writerow(dataset_array[index_shuffle[count]])\n",
    "    count += 1    \n",
    "csvfile.close()\n",
    "\n",
    "print(\"------ Train----------\")\n",
    "print(get_number_of_lines_in_file(fileNameTrain))\n",
    "print(\"------ Test-------\")\n",
    "print(get_number_of_lines_in_file(fileNameTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Putting together all train and test files\n",
    "So far, we have the files as shown in the next Figure. One folder contains the training data and another the test data. <br>\n",
    "Each folder of training or test data consist so far in files from each class containg all the vectors from that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train_test_classesFiles.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to put them all in a single file containing all data from all classes. However, we don't want that the rows will be in order, but we want to have mixed rows from every class. The next Figure shows in the left an example of how would it be a file containing the classes in order with a respective index. In the right side it shows how would it be a file with the data mixed from all the classes and a respective shuffled index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/whole_shuffled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we do is to create a list with the index of all vectors and then shuffle it. We write row by row in a new file by reading each class file depending in the shuffled list as depicted in the next Figure. We do this with train and test separatedely. In this case, the idea is to avoid to load in memory all data because it can be a huge amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/step_by_step_write.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code used os.walk to walk among directories and files contained in a root path. More information in here <br>\n",
    "https://docs.python.org/3/library/os.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- test\n",
      "------ class_0.csv\n",
      "------ class_1.csv\n",
      "------ class_2.csv\n",
      "[0, 1, 2]\n",
      "['class_0.csv', 'class_1.csv', 'class_2.csv']\n",
      "[6, 6, 6]\n",
      "--- train\n",
      "------ class_0.csv\n",
      "------ class_1.csv\n",
      "------ class_2.csv\n",
      "[0, 1, 2]\n",
      "['class_0.csv', 'class_1.csv', 'class_2.csv']\n",
      "[14, 14, 14]\n"
     ]
    }
   ],
   "source": [
    "#This function get information about how much files are within a folder and how many elements does each file has. \n",
    "#It also return the name of the files as Classes names (Recall that all information about one class is now in one file)\n",
    "def get_info_folder_with_classes(pathFile):\n",
    "    i=0\n",
    "    Classes = []\n",
    "    ClassesNames = []\n",
    "    nRowsInClasses = []\n",
    "    for root, dirs, files in os.walk(pathFile):\n",
    "        path = root.split(os.sep)\n",
    "        print((len(path) - 1) * '---', os.path.basename(root))\n",
    "        for file in files:\n",
    "            Classes.append(i)\n",
    "            i +=1\n",
    "            ClassesNames.append(file)\n",
    "            print(len(path) * '---', file)\n",
    "            nRows = get_number_of_lines_in_file(os.path.join(root,file))\n",
    "            nRowsInClasses.append(nRows)\n",
    "    return Classes, ClassesNames, nRowsInClasses\n",
    "\n",
    "\n",
    "#This function writes in a file the shuffled data from all classes. Reads row by row according to a shuffling of the classes.\n",
    "def write_shuffled_classes(folderWithFiles,folderToSave,Classes,ClassesNames,nRowsInClasses):\n",
    "    #Files to be opened\n",
    "    fileNameToSave = os.path.join(folderToSave,\"all_Shuffled_\"+dirName+\".csv\")\n",
    "    fileIndexClass = os.path.join(folderToSave,\"classesIndex_\"+dirName+\".csv\")\n",
    "    fileClassNames = os.path.join(folderToSave,\"classesNames_\"+dirName+\".csv\")\n",
    "    \n",
    "    #File to write all data comming from all classes\n",
    "    csvFile = open(fileNameToSave, 'wt',newline='') \n",
    "    Shuffle_data_writer = csv.writer(csvFile, delimiter='\\t')\n",
    "    \n",
    "    #File to write to which class each rows corresponds to\n",
    "    csvIndex = open(fileIndexClass, 'wt',newline='') \n",
    "    NameIx_data_writer = csv.writer(csvIndex, delimiter='\\t')\n",
    "    \n",
    "    #File to write the name of the classes\n",
    "    csvNameCl = open(fileClassNames, 'wt') \n",
    "    \n",
    "    #Makes a list of the number of elements of the index of each class\n",
    "    index = np.array([])\n",
    "    for i in range(len(Classes)):\n",
    "        index = np.append(index,np.ones((1,nRowsInClasses[i]))*Classes[i])\n",
    "        csvNameCl.write(ClassesNames[i]+\"\\n\")\n",
    "        \n",
    "    #Shuffle the index\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    #Makes a list of fileObjects to write depending of the shuffled index\n",
    "    fileObjectsReader = []\n",
    "    for i in range(len(Classes)):\n",
    "        fileObjectsReader.append(csv.reader(open(os.path.join(folderWithFiles,ClassesNames[i])),delimiter = \"\\t\"))\n",
    "        \n",
    "    #Writes the whole data step by step getting data from a given class depending in the shuffles list\n",
    "    for i in range(len(index)):\n",
    "        Shuffle_data_writer.writerow(next(fileObjectsReader[int(index[i])]))\n",
    "        NameIx_data_writer.writerow(str(int(index[i])))\n",
    "        \n",
    "    #Closes all files\n",
    "    csvFile.close()\n",
    "    csvIndex.close()\n",
    "    csvNameCl.close()\n",
    "    \n",
    "        \n",
    "        \n",
    "#The path with the folders having the data (traind and test)\n",
    "folderWithFiles=\"prueba/\"\n",
    "#The name of the folder where the data will be sabed\n",
    "folderToSave = folderWithFiles + \"all_train_and_test\"\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(folderWithFiles):\n",
    "    if root != folderWithFiles and root!= folderToSave:  #To avoid reading the files from the root and the folder where the data will be saved\n",
    "        pathRoot = root.split('/') #root.split(os.sep) depending on how to separate path {\\\\,/}\n",
    "        dirName = os.path.basename(root)\n",
    "        Classes, ClassesNames, nRowsInClasses = get_info_folder_with_classes(os.path.join(pathRoot[0],dirName))\n",
    "        print(Classes)\n",
    "        print(ClassesNames)\n",
    "        print(nRowsInClasses)\n",
    "        write_shuffled_classes(os.path.join(pathRoot[0],dirName),folderToSave,Classes,ClassesNames,nRowsInClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Write hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "#Path and filenames\n",
    "path_data = \"prueba/all_train_and_test/\"\n",
    "fileNameTest_x = \"all_Shuffled_test.csv\"\n",
    "fileNameTest_y = \"classesIndex_test.csv\"\n",
    "\n",
    "#Read the datasets\n",
    "dataset = load_csv(os.path.join(path_data,fileNameTest_x))\n",
    "dataset_array = str_dataset_2_numpy_dataset(dataset)\n",
    "\n",
    "datasetY = load_csv(os.path.join(path_data,fileNameTest_y))\n",
    "datasetY_array = str_dataset_2_numpy_dataset(datasetY)\n",
    "\n",
    "#Creates a hdf5 file\n",
    "fileName_hdf5 = \"prueba/all_train_and_test/My_test_file_.hdf5\"\n",
    "f = h5py.File(fileName_hdf5, \"w\")\n",
    "\n",
    "#Creates datasets HDF5\n",
    "dset = f.create_dataset(\"test_data\", data=dataset_array)\n",
    "dsetY =  f.create_dataset(\"test_data_Y\", data=datasetY_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 10)\n",
      "(18, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = h5py.File(fileName_hdf5, \"r\")\n",
    "\n",
    "test_set_x_orig = np.array(dataset[\"test_data\"][:]) # your test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_data_Y\"][:]) # your test set labels\n",
    "\n",
    "print(test_set_x_orig.shape)\n",
    "print(test_set_y_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
